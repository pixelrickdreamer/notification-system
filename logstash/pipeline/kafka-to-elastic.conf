input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["alerts", "orders.events", "payments.events", "inventory.events"]
    group_id => "logstash-consumer"
    codec => json
    auto_offset_reset => "earliest"
  }
}

filter {
  # Add a timestamp if not present
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }

  # Parse the event type for easier filtering in Kibana
  if [type] {
    mutate {
      add_field => { "event_category" => "%{type}" }
    }

    # Extract the main category (e.g., "order" from "order.created")
    grok {
      match => { "type" => "^%{WORD:event_domain}\.%{GREEDYDATA:event_action}" }
      tag_on_failure => ["_grok_parse_failure"]
    }
  }

  # Add metadata
  mutate {
    add_field => { "pipeline" => "kafka-to-elastic" }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "events-%{+YYYY.MM.dd}"
  }

  # Also output to stdout for debugging
  stdout {
    codec => rubydebug
  }
}
